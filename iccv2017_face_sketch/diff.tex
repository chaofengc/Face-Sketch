\documentclass[10pt,twocolumn,letterpaper]{article}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL sketch_revised_v1.tex   Sun Mar 12 13:42:38 2017
%DIF ADD sketch_draft.tex        Sun Mar 12 13:43:43 2017

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{subfigure}
%DIF 11a11-12
\usepackage{multirow} %DIF > 
\usepackage{array} %DIF > 
%DIF -------

% Include other packages here, before hyperref.
%DIF 13c15-16
%DIF < 
%DIF -------
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp} %DIF > 
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}} %DIF > 
%DIF -------
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\def\comm[#1]{{\small \textcolor{red}{\emph{#1}}}}
\def\red[#1]{\textcolor{red}{\textbf{#1}}}
\def\redn[#1]{\textcolor{red}{#1}}
\def\blue[#1]{\textcolor{blue}{#1}}
\def\green[#1]{\textcolor{green}{#1}}
\def\revise[#1]{{\small \textcolor{blue}{\emph{}}}}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{621} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

%%%%%%%%% TITLE
\title{Face \DIFdelbegin \DIFdel{Sketch Synthesis by Style Transfer }\DIFdelend \DIFaddbegin \DIFadd{Sketching }\DIFaddend with \DIFdelbegin \DIFdel{Local Features}\DIFdelend \DIFaddbegin \DIFadd{Deep Neural Networks}\DIFaddend }

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

\DIFdelbegin \DIFdel{Face sketch synthesis is challenging as it is difficult to generate sharp and detailed textures. }\DIFdelend In this paper, we propose a new \DIFaddbegin \DIFadd{face sketch synthesis }\DIFaddend framework based on deep neural networks. Imitating the process of how artists draw sketches, our framework synthesizes face sketches in a cascaded manner in which a content image is first generated that outlines the shape of the face and key facial features, and textures and shadings are then added. \DIFdelbegin \DIFdel{We utilize a Fully Convolutional Neural Network (FCNN) to create the content image, and propose a local feature based style transfer to append textures . The local feature, what we call pyramid column feature, is a set of features at different convolutional layers corresponding to the same local sketch image patch. We demonstrate that our pyramid column feature can not only preserve more sketch details than common style transfer method but also surpass traditional patch based approach. Our model is trained on }%DIFDELCMD < \red[??] %%%
\DIFdel{training data set and evaluated on other }\DIFdelend \DIFaddbegin \DIFadd{By exploiting the capability of deep networks for generating beautiful textures and stylized images, our framework can produce sketches that look like sketches drawn by real artists in strokes. Experiments are carried out on both the CUHK and AR }\DIFaddend datasets. Quantitative and qualitative evaluations suggest that our framework outperforms other state-of-the-arts methods.
\DIFdelbegin \DIFdel{In addition, despite of the small training data (}%DIFDELCMD < \red[??] %%%
\DIFdel{face-sketch pairs), our model shows great generalization ability across different datasets and can generate reasonable results under practical situations.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \end{abstract}

%%%%%%%%% BODY TEXT
%DIF < ==========================================================================
\DIFaddbegin {
\DIFaddend \section{Introduction}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin }
\DIFaddend Face sketch synthesis has drawn a great attention from the community in recent years because of its wide range of applications. For instance, it can be exploited in law enforcement for identifying suspects from a mug shot database consisting of both photos and sketches. Besides, face sketch has also been widely used for entertainment purpose. For example, filmmakers could employ face sketch synthesis technique to ease the cartoon production process. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < \begin{figure}[t]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_photo.png}
%DIFDELCMD < %%%
\DIFdelFL{(a) Photo
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_mrf.png}
%DIFDELCMD < %%%
\DIFdelFL{(b) MRF\mbox{%DIFAUXCMD
\cite{wang2009face}
}%DIFAUXCMD
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_wmrf.png}
%DIFDELCMD < %%%
\DIFdelFL{(c) WMRF\mbox{%DIFAUXCMD
\cite{zhou2012markov}
}%DIFAUXCMD
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_ssd.png}
%DIFDELCMD < %%%
\DIFdelFL{(d) SSD\mbox{%DIFAUXCMD
\cite{song2014real}
}%DIFAUXCMD
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_fcnn.png}
%DIFDELCMD < %%%
\DIFdelFL{(e) FCNN\mbox{%DIFAUXCMD
\cite{zhang2015end}
}%DIFAUXCMD
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.23\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_bfcn.png}
%DIFDELCMD < %%%
\DIFdelFL{(f) BFCN \mbox{%DIFAUXCMD
\cite{zhang2017content}
}%DIFAUXCMD
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_deepart.jpg}
%DIFDELCMD < %%%
\DIFdelFL{(h) \mbox{%DIFAUXCMD
\cite{gatys2015neural}}%DIFAUXCMD
$^*$
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{0.24\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\linewidth]{img/example_ours.png}
%DIFDELCMD < %%%
\DIFdelFL{(g) Ours
}%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{1\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_photo_patch.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_mrf_patch.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_wmrf_patch.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_ssd_patch.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_fcnn_patch.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_bfcn_patch.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_deepart_patch.jpg}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/hairpin_ours_patch.png}
%DIFDELCMD < \end{minipage}
%DIFDELCMD < \begin{minipage}[t]{1\linewidth}
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_photo.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_mrf.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_wmrf.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_ssd.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_fcnn.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_bfcn.png}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_deepart.jpg}
%DIFDELCMD < \includegraphics[width=0.11\linewidth]{img/eye_ours.png}
%DIFDELCMD < \end{minipage}
%DIFDELCMD < %%%
%DIFDELCMD < \caption[Caption for LOF]{%
{%DIFAUXCMD
\DIFdelFL{Face sketches generated by existing methods and the proposed method. Our method can not only preserve both hair and facial content, but also maintain sharp textures. }%DIFDELCMD < \setcounter{footnote}{0} %%%
\DIFdelFL{(h)$^*$ is obtained from deep art website}%DIFDELCMD < \footnotemark%%%
\DIFdelFL{~by using the photo as content and a sketch from training set as style.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:example_comp}
%DIFDELCMD < \end{figure}
%DIFDELCMD < %%%
\footnotetext{%DIFDELCMD < \url{https://deepart.io/}%%%
} 
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \par
\DIFaddend Unfortunately, there exists no easy solution to face sketch synthesis due to the big stylistic gap between photos and sketches. In the past two decades, a number of exemplar based methods~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{wang2009face,song2014real, zhang2010lighting,zhou2012markov} }%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{wang2009face,zhang2010lighting,zhou2012markov} }%DIFAUXCMD
}\DIFaddend were proposed. In these methods, an input photo is divided into patches and candidate sketches for each photo patch are selected from a training set. The \DIFdelbegin \DIFdel{main drawback of such kind of methods is that if the test image can't find a similar patch in the training set, they may lose some contents in the final result. For example, the sketches in the first row of Fig.\ref{fig:example_comp} fail to keep the hairpins. Besides, some methods \mbox{%DIFAUXCMD
\cite{song2014real,zhou2012markov} }%DIFAUXCMD
clear away the textures when they try to eliminate the }\DIFdelend \DIFaddbegin \DIFadd{sketch output of a photo patch can be represented by either the best candidate or a linear combination of the best $K$ candidate sketch patches. However, processing each photo patch individually often introduces }\DIFaddend inconsistency between neighboring patches. \DIFdelbegin \DIFdel{Another potential risk is that the result may not look like the original photo, }%DIFDELCMD < \eg %%%
\DIFdel{left eye inFig.~\ref{fig:example_comp} (b) }\DIFdelend \DIFaddbegin \DIFadd{To tackle this problem, methods in~\mbox{%DIFAUXCMD
\cite{wang2009face,zhang2010lighting,zhou2012markov} }%DIFAUXCMD
adopted Markov Random Fields (MRF) to regularize the synthesis process}\DIFaddend . Recently, \DIFdelbegin \DIFdel{approaches \mbox{%DIFAUXCMD
\cite{zhang2017content,zhang2015end} }%DIFAUXCMD
based on }\DIFdelend \DIFaddbegin \DIFadd{an image based approach~\mbox{%DIFAUXCMD
\cite{song2014real} }%DIFAUXCMD
based on the idea of joint image filtering was proposed. It was found superior in suppressing noises in the sketch. Another recent approach based on a feed-forward }\DIFaddend convolutional neural network (\DIFdelbegin \DIFdel{CNN) were developed to solve these problems. Since these models directly generates sketches from photo, they can maintain the structures and contents of the photos. However, the loss function of them are usually mean square error (MSE) or variation of it, which is responsible for the blur effect, }%DIFDELCMD < \eg %%%
\DIFdel{Fig. ~\ref{fig:example_comp} (e) and (f). The }\DIFdelend \DIFaddbegin \DIFadd{FCNN) was found to be more effective in generating discriminative facial details~\mbox{%DIFAUXCMD
\cite{zhang2015end}}%DIFAUXCMD
. Although sketches produced by these methods do look like the faces in the photos, they look quit different from sketches drawn by real artists. One major }\DIFaddend reason is that \DIFdelbegin \DIFdel{MSE prefers values close to mean, and is not suitable for texture representations. The popular neural style transfer provides a better solution for texture synthesis. But there are two obstacles towards directly applying such kind of method. First, it is easily influenced by illumination of the photo, see the face of Fig.\ref{fig:example_comp} (h). Second, it needs a style image to give the global statistics of textures. If the given style doesn't coincide with target sketch (which we don't have), some side effects will occur, }%DIFDELCMD < \eg %%%
\DIFdel{the nose }\DIFdelend \DIFaddbegin \DIFadd{these methods do not consider the style of the synthesized sketches. For example, the hair in their synthesized sketches is often lack of texture, and the region around the face boarders often suffers from blurring artifacts (see Fig.~\ref{fig:example_comp}). Besides, both the exemplar based and image based approaches cannot handle structures that do not exist in the training set (e.g., clothes }\DIFaddend in Fig.\DIFdelbegin \DIFdel{\ref{fig:example_comp} (h).Extensive experiment and discussion is given in Section~}%DIFDELCMD < \red[??]%%%
\DIFdel{. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{~\ref{fig:example_comp}).}\par
%DIF >  \begin{figure}[t]
%DIF >  \centering
%DIF >  \begin{minipage}[t]{0.24\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=1\linewidth]{example_ssd.pdf}
%DIF >  \cite{song2014real}
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.24\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=1\linewidth]{example_mrf.pdf}
%DIF >  \cite{wang2009face}
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.24\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=1\linewidth]{example_wmrf.pdf}
%DIF >  \cite{zhou2012markov}
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.24\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=1\linewidth]{example_ours.pdf}
%DIF >  Proposed
%DIF >  \end{minipage}
%DIF >  \caption{Face sketches generated by existing methods and the proposed method. The proposed method can achieve a better performance than the exemplar based methods~\cite{wang2009face,zhou2012markov} and the image based method~\cite{song2014real} in appearance.}
%DIF >  \label{fig:example_comp}
%DIF >  \end{figure}
\DIFadd{Inspired by the way how artists draw sketches, we propose a new framework for face sketch synthesis that can overcome the aforementioned limitations. }\DIFaddend For an artist, \DIFaddbegin \DIFadd{instead of drawing sketches region by region, }\DIFaddend the procedure of sketching a face usually starts with outlining the shape of the key facial features like the nose, eyes and mouth. Textures and shadings are then added to regions such as hair lips, and bridge of the nose to give sketches a specific style. \DIFdelbegin \DIFdel{Inspired by this and neural style transfer \mbox{%DIFAUXCMD
\cite{gatys2015texture}}%DIFAUXCMD
, we propose a new framework for face sketch synthesis that can overcome the aforementioned limitations. }\DIFdelend In our method, the outline of a face is delineated by a feed-forward neural network, and textures and shadings are then added by a style transfer approach.  \DIFdelbegin \DIFdel{Specifically, we design a new architecture of Fully Convolutional Neural Network (FCNN) which contains inception layers \mbox{%DIFAUXCMD
\cite{szegedy2015going} }%DIFAUXCMD
and convolution layers with batch normalization~\mbox{%DIFAUXCMD
\cite{Sergey2015batch} }%DIFAUXCMD
to outline the face (Section ~}%DIFDELCMD < \red[??]%%%
\DIFdel{). For the texture part, we first }\DIFdelend \DIFaddbegin \DIFadd{We formulate the style as a statistical measure of the features extracted from multiple resolutions of the sketches using the VGG-Network~\mbox{%DIFAUXCMD
\cite{simonyan2014very}}%DIFAUXCMD
. Due to the non-linearity of these features and the fact that there usually does not exist a training photo that is very similar to the test photo, estimating the style for a target sketch is not easy. In spired by the work of Wang and Tang~\mbox{%DIFAUXCMD
\cite{wang2009face}}%DIFAUXCMD
, we }\DIFaddend divide the feature maps of the target sketch \DIFdelbegin \DIFdel{in each layer into a fixed size grid and combine features from different layers but at the same grid location into a pyramid feature column (Section ~}%DIFDELCMD < \red[??]%%%
\DIFdel{). These pyramid feature columns can be generated by local sketch patches from the training set}\DIFdelend \DIFaddbegin \DIFadd{into a grid and use the features of a candidate sketch patch as a surrogate for the feature map patch of a grid cell}\DIFaddend . A target style is then computed \DIFdelbegin \DIFdel{by assembling these pyramid columns. These sketch patches are found by matching the test }%DIFDELCMD < \red[content] %%%
\DIFdel{patch to the }%DIFDELCMD < \red[content-sketch] %%%
\DIFdel{pairs in training set(Section ~}%DIFDELCMD < \red[??]%%%
\DIFdel{)}\DIFdelend \DIFaddbegin \DIFadd{from the resulting feature maps}\DIFaddend . Our approach is superior to the current state-of-the-art methods in that \DIFdelbegin %DIFDELCMD < \begin{itemize}
%DIFDELCMD < \item %%%
\DIFdel{It }\DIFdelend \DIFaddbegin \DIFadd{(1) it }\DIFaddend is capable of generating more stylistic sketches without introducing over smoothing artifacts \DIFdelbegin %DIFDELCMD < \item %%%
\DIFdel{It can well preserve the content of the test photo.}%DIFDELCMD < \end{itemize}
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < ==========================================================================
\DIFdelend \DIFaddbegin \DIFadd{and (2) it can draw structures which do not exist in the training set.}\par
{
\DIFaddend \section{Related Work}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin }
{
\DIFaddend \subsection{Face Sketch Synthesis}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin }
\DIFaddend Based on the taxonomy of previous studies~\cite{song2014real,zhou2012markov}, face sketch synthesis methods can be roughly categorized into profile sketch synthesis methods~\cite{berger2013style,chen2001example,xu2008hierarchical} and shading sketch synthesis methods~\cite{liu2005nonlinear,song2014real,tang2003face,wang2009face,zhang2015end,zhang2010lighting,zhou2012markov}. Compared with profile sketches, shading sketches are more expressive and thus more preferable in practice. Based on the assumption that there exists a linear transformation between a face photo and a face sketch, the method in~\cite{tang2003face} computes a global eigen-transformation for synthesizing face sketches from face photos. This assumption, however, does not always hold since the modality of face photos and that of face sketches are quite different. Fortunately, Liu et al.~\cite{liu2005nonlinear} found that the linear transformation holds better locally and therefore they proposed a patch based method to perform sketch synthesis. A MRF based method~\cite{wang2009face} was proposed to preserve large scale structures across sketch patches. Variants of the MRF based methods were introduced in~\cite{zhang2010lighting,zhou2012markov} to improve the robustness to lighting and pose, and to render the ability of generating new sketch patches. In addition to these MRF based methods, approaches based on guided image filtering~\cite{song2014real} and feed-forward convolutional neural network~\cite{zhang2015end} are also found to be effective in transferring photos into sketches.
\DIFdelbegin \DIFdel{A very recent work similar to ours is done by Zhang }%DIFDELCMD < \etal %%%
\DIFdel{\mbox{%DIFAUXCMD
\cite{zhang2017content}}%DIFAUXCMD
. They proposed a two branch FCNN to learn content and texture respectively and then fusion them through a face probability map. Although their results are impressing, the sketch texture is not natural and the facial components are smoothed. }%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < ------------------------------------------------------------------------
\subsection{\DIFdel{Style Transfer with CNN}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Texture synthesis has long been a challenging task. Traditional method can only imitate repetitive patterns which has a strong limitation. Recently, Gatys }%DIFDELCMD < \etal %%%
\DIFdelend \DIFaddbegin {
\subsection{\DIFadd{Style Transfer with Convolution Neural Network}}
}
\DIFadd{The class of Convolutional Neural Networks (CNN) is perhaps the most powerful tool in image processing. It usually contains layers of filters each of which extracts a certain feature from the input or from the output of the previous layer. The VGG-Network~\mbox{%DIFAUXCMD
\cite{simonyan2014very}}%DIFAUXCMD
, one popular instance of such networks, rivals human performance in image classification tasks. This demonstrates the ability of CNN in feature extraction. In}\DIFaddend ~\cite{gatys2015texture,gatys2015neural}, \DIFaddbegin \DIFadd{Gatys et al. }\DIFaddend studied the use of CNN in style representation \DIFdelbegin \DIFdel{(including texture and color) }\DIFdelend where a target style is computed based on features extracted from an image using the VGG-Network and an output image is generated by minimizing the difference between its style and the target style. \DIFdelbegin \DIFdel{It can transfer any style to any images, and the results are impressive. Justin }%DIFDELCMD < \etal %%%
\DIFdel{\mbox{%DIFAUXCMD
\cite{feifei2016} }%DIFAUXCMD
further accelerated this process by learning a feed forward CNN in the training stage. These methods represent textures by a multi-scale gram matrix of feature maps. Since gram matrix cares more about global statistics, if the style is very different from the photo, it usually breaks the local structures. Although it is not a big deal inartistic style, it can't be tolerated in face sketch synthesis. In \mbox{%DIFAUXCMD
\cite{Chen2016Patch}}%DIFAUXCMD
, Chen and Schmidt propose a different patch based style transfer method which is better at capture local structures. However, it is still not suitable for this task. }\DIFdelend \DIFaddbegin \DIFadd{Likewise, a perceptual loss function measuring the difference in style between a targeting image and images generated from a CNN was proposed in~\mbox{%DIFAUXCMD
\cite{feifei2016} }%DIFAUXCMD
and it was then exploited in the CNN training stage. }\DIFaddend Our style transfer mechanism is inspired by but different from these works~\cite{gatys2015texture,gatys2015neural,feifei2016} in that our target style is extracted from many \DIFdelbegin \DIFdel{image patches }\DIFdelend \DIFaddbegin \DIFadd{images }\DIFaddend rather than from a single style image. Note that there usually does not exist a single style image in the training set that matches all properties of the test image. \DIFaddbegin \DIFadd{Hence, we propose computing the target style based on multiple images. The difficulty of generating a target style from multiple images lies in the non-linearity of the neural network.
%DIF > -------------------------------------------------------------------------
}\DIFaddend 

\DIFdelbegin %DIFDELCMD < \begin{figure*}[t]
%DIFDELCMD < \centering
%DIFDELCMD < \subfigure[]{
%DIFDELCMD < \includegraphics[width=0.85\linewidth]{img/overview.pdf}
%DIFDELCMD < }
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{The proposed method contains two branches which take an photo aligned by the eyes as inputs. The content network outputs a content image and the style estimator generates a target style. The final sketch is generated by combing the target style with the content image. }%DIFDELCMD < \comm[The figure may need to revise, in order to show the optimization process of target sketch.]%%%
}
%DIFAUXCMD
%DIFDELCMD < \label{fig:overview}
%DIFDELCMD < \end{figure*}
%DIFDELCMD < %%%
%DIF < ==========================================================================
\section{\DIFdel{Motivation of Pyramid Feature Column}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Following the practice of \mbox{%DIFAUXCMD
\cite{gatys2015neural}}%DIFAUXCMD
, we use the gram matrix of VGG-19\mbox{%DIFAUXCMD
\cite{simonyan2014very} }%DIFAUXCMD
feature maps as our style representation. Denote the vectorized feature map of the final sketch $\mathcal{X}$ in the $l$th layer by $F^{l}(\mathcal{X})$. A gram matrix is the inner product between the feature maps in $l$th layer
}\begin{displaymath}
\DIFdel{G^l_{ij}(\mathcal{X}) = \sum \limits_{k=1}^{M_l} F^l_{ik}(\mathcal{X}) F^l_{jk}(\mathcal{X})
\label{eq:Gram_element}
}\end{displaymath}
%DIFAUXCMD
\DIFdel{where $G^l(\mathcal{X}) \in {\mathcal{R}^{N_l \times N_l}}$, $M_l$ is the height times width of the feature map $F^{l}(\mathcal{X})$, and $N_l$ is the number of feature maps in the $l$th layer. Since $G^l_{ij}(\mathcal{X})$ is an inner product of feature maps, a gram matrix is actually a summary statistics of feature maps discarding the spatial information. Although we still not clear what these values exactly mean, we can safely make an assumption that it at least captures the density distribution of a sketch. In other word, if the given sketch style has much less hair than the test image, the generated sketch $\mathcal{X}$ will possibly be unnaturally bright than a natural sketch. Experiments in Section~}%DIFDELCMD < \red[??] %%%
\DIFdel{also prove this. 
Thus it is important to keep the given style sketch roughly the same with test image statistically. On the other hand, there usually does not exist a candidate image in the training set that perfectly matches a given test photo in style. We hence propose a feature level patch based method to estimate the style of the final sketch.
Each feature patch corresponds to a sketch patch. The reason why we can separate features into patches comes from \mbox{%DIFAUXCMD
\cite{Li2017Demistify}}%DIFAUXCMD
. The feature vectors at different position of feature map can be viewed as independent samples when we use gram matrix. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < ==========================================================================
\DIFdelend \DIFaddbegin {
\DIFaddend \section{\DIFdelbegin \DIFdel{Methodology}\DIFdelend \DIFaddbegin \DIFadd{Overview of Our Method}\DIFaddend }
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin }
%DIF >  \begin{figure*}[t]
%DIF >  \centering
%DIF >  \subfigure[]{
%DIF >  \includegraphics[width=0.85\linewidth]{overview.pdf}
%DIF >  }
%DIF >  \caption{The proposed method contains two branches which take an photo aligned by the eyes as inputs. The content network outputs a content image and the style estimator generates a target style. The final sketch is generated by combing the target style with the content image.}
%DIF >  \label{fig:overview}
%DIF >  \end{figure*}
\DIFaddend Our method can be classified as a shading synthesis method. The steps of our method are summarized in Fig.~\ref{fig:overview}. First, a preprocessing step as described in~\cite{wang2009face} is carried out for all photos and sketches in a training set to align the centers of two eyes. A test photo $\mathcal{I}$ is then fed into two branches, namely the content network and the style generator\DIFaddbegin \DIFadd{, respectively}\DIFaddend . The content network converts the test photo into a content image $\mathcal{C}$, where \DIFaddbegin \DIFadd{facial features and }\DIFaddend the shape of the face are outlined with the \DIFdelbegin \DIFdel{key }\DIFdelend \DIFaddbegin \DIFadd{global arrangement of the }\DIFaddend facial features preserved\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{. It is not necessary that the style of the content image resembles a real sketch, but the shapes of different features }\DIFaddend such as noses, eyes, mouthes and hair\DIFaddbegin \DIFadd{, should be delineated}\DIFaddend . The style estimator takes \DIFdelbegin \DIFdel{a $16\times16$ local patch from }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend test photo as input and \DIFdelbegin \DIFdel{searches }\DIFdelend \DIFaddbegin \DIFadd{utilizes }\DIFaddend photo-sketch pairs \DIFdelbegin \DIFdel{(}%DIFDELCMD < \comm[Maybe we can find a content-sketch pairs instead, because real photo varies too much]%%%
\DIFdel{) }\DIFdelend in the training set to \DIFdelbegin \DIFdel{find a target sketch patch $S_{ij}$ ($(i, j)$ denotes the patch location). Each  $S_{ij}$ with its surrounding region can generate a pyramid feature column $U_{ij}$. Combining all $U_{ij}$, we can get the target style features of $\mathcal{I}$, }%DIFDELCMD < \ie %%%
\DIFdel{$\tilde{U}$}\DIFdelend \DIFaddbegin \DIFadd{generate a target style $\mathcal{S}$ which we want to transfer to the final sketch}\DIFaddend . Given $\mathcal{C}$ and \DIFdelbegin \DIFdel{$\tilde{U}$, we can }\DIFdelend \DIFaddbegin \DIFadd{$\mathcal{S}$, we }\DIFaddend generate a sketch $\mathcal{X}$ that combines the content information in $\mathcal{C}$ with the style representation \DIFdelbegin \DIFdel{$\tilde{U}$ following the iterative procedure in \mbox{%DIFAUXCMD
\cite{gatys2015neural}}%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < ------------------------------------------------------------------------
\subsection{\DIFdel{Content Image Generation}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < \begin{figure*}[htbp]
%DIFDELCMD < \centering
%DIFDELCMD < \subfigure[]{
%DIFDELCMD < \includegraphics[width=0.65\linewidth]{img/content_net.pdf}
%DIFDELCMD < }
%DIFDELCMD < \subfigure[]{
%DIFDELCMD < \includegraphics[width=0.25\linewidth]{img/inception.pdf}
%DIFDELCMD < }
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Illustration of the content network for generating a content image. The numbers above the building block denote the number of CNN filters. (a) The architecture of content network. (b) The inception module in (a) contains three groups of filters with different sizes.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:content_NN}
%DIFDELCMD < \end{figure*}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Our content network architecture is shown in Fig.~\ref{fig:content_NN}}\DIFdelend \DIFaddbegin \DIFadd{$\mathcal{S}$. This is formulated as an optimization problem over an energy function taking both $\mathcal{C}$ and $\mathcal{S}$ into considerations. Details will be explained in the following subsections.
}{
\section{\DIFadd{Content Image Generation}}
}
\DIFadd{We use CNN to build our content network (see Fig~\ref{fig:content_NN})}\DIFaddend . In addition to the test photo, \DIFdelbegin \DIFdel{we feed two extra }\DIFdelend \DIFaddbegin \DIFadd{two location }\DIFaddend channels containing spatial information (i.e., x and y coordinates) and a difference of Gaussian (DoG) \DIFdelbegin \DIFdel{image }\DIFdelend \DIFaddbegin \DIFadd{channel containing edge information are fed }\DIFaddend into the content network. As pointed out in~\cite{wang2009face}, face sketch synthesis algorithms benefit from integrating features from multiple resolutions. We employ an inception module inspired by the GoogLeNet~\cite{szegedy2015going} to extract features. It concatenates feature maps generated from filters with different spatial resolutions. Our inception unit contains \DIFaddbegin \DIFadd{$3$ CNN filter groups, with each group having $32$ filters. The filters in these }\DIFaddend 3 \DIFdelbegin \DIFdel{different size of filters $(1\times1)$, $(3\times3)$ and $(5\times5)$ }\DIFdelend \DIFaddbegin \DIFadd{groups have a size of $autoref(1\times1)$, $autoref(3\times3)$ and $autoref(5\times5)$ respectively }\DIFaddend (see Fig. \DIFdelbegin \DIFdel{\ref{fig:content_NN}}\DIFdelend \DIFaddbegin \DIFadd{3}\DIFaddend (b)). \DIFdelbegin \DIFdel{Then, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend output features are \DIFaddbegin \DIFadd{then }\DIFaddend fed to a three-layer-CNN for feature integration, where the size of all filters are fixed at \DIFdelbegin \DIFdel{$1\times1$}\DIFdelend \DIFaddbegin \DIFadd{$autoref(1\times1)$}\DIFaddend . Finally, the integrated features are used to reconstruct the content map by a two-layer-CNN with the filter size being $3\times3$. A mirror padding is carried out before the convolution operation when necessary to ensure the output feature map is the same size as the input. \DIFdelbegin \DIFdel{The output content image $\mathcal{C}$ is a gray image with size $250\times256$.
}\DIFdelend \DIFaddbegin \DIFadd{It is observed that the output of the content net outlines the shapes of the facial features, but its style does not look like sketches. For example, there exists no texture in the hair region and there is a lack of shadings to convey 3D information.
%DIF >  \begin{figure*}[htbp]
%DIF >  \centering
%DIF >  \subfigure[]{
%DIF >  \includegraphics[width=0.65\linewidth]{content_net.pdf}
%DIF >  }
%DIF >  \subfigure[]{
%DIF >  \includegraphics[width=0.25\linewidth]{inception.pdf}
%DIF >  }
%DIF >  \caption{Illustration of the content network for generating a content image. The numbers above the building block denote the number of CNN filters. (a) The architecture of content network. (b) The inception module in (a) contains three groups of filters with different sizes.}
%DIF >  \label{fig:content_NN}
%DIF >  \end{figure*}
}\DIFaddend 

\DIFaddbegin {
\section{\DIFadd{Sketch Style Generation}}
}
{
\DIFaddend \subsection{\DIFdelbegin \DIFdel{Pyramid Feature Column}\DIFdelend \DIFaddbegin \DIFadd{Style Representation}\DIFaddend }
%DIF < ------------------------------------------------------------------------
\DIFaddbegin }
\DIFadd{The style features of sketches are extracted using the VGG-Network~\mbox{%DIFAUXCMD
\cite{simonyan2014very}}%DIFAUXCMD
, a CNN that was originally designed for visual object recognition tasks, and was later used for texture feature representation in~\mbox{%DIFAUXCMD
\cite{gatys2015texture}}%DIFAUXCMD
. Here we use the intermediate feature maps extracted by the convolutional layers and max-pooling layers in the VGG-19 network just before the fully connected layers. 
}\DIFaddend 

\DIFdelbegin \DIFdel{Fig. \ref{fig:pyramidcolumn} shows an example of the pyramid feature column. }\DIFdelend \DIFaddbegin \DIFadd{In the VGG-Network, convolutional layers extract features and max-pooling layers down-sample the features such that the following convolutional layers extract features in a lower resolution. We extract the outputs of the first convolutional layers at different resolutions for the computation of style. Following the terminology in~\mbox{%DIFAUXCMD
\cite{gatys2015texture}}%DIFAUXCMD
, these layers are referred to as ``conv1\_1'', ``conv2\_1'', ``conv3\_1'', ``conv4\_1'', and ``conv5\_1'', respectively, from the lowest level to the highest level. Define $L_s$ $=$ \{conv1\_1, conv2\_1, conv3\_1, conv4\_1, conv5\_1\}. Since the VGG-Network is originally designed for color images, while sketches are gray scale images, we modify the first layer of VGG-Network for gray scale images by setting the filter weights to
}\begin{equation}
\DIFadd{W^{k} = W^{k}_r+W^{k}_g+W^{k}_b
\label{eq:VGG_weights}
}\end{equation}
\DIFadd{where $W^{k}_r$, $W^{k}_g$, and $W^{k}_b$ are weights of the $k$th filter in the first convolutional layer for the R, G and B channels respectively, and $W^{k}$ is the weight of the $k$th filter in the first convolutional layer of our modified network. Note that we keep the bias in our modified network the same as that in the original VGG-Network. Denote the feature map of the final sketch $\mathcal{X}$ in the $l$th layer (in matrix form) by $F^{l}autoref(\mathcal{X})$, where the element at the $m$th row and the $n$th column is the activation of the $m$th filter at location $n$. We use the gram matrix as our style representation. A gram matrix is a summary statistics of feature maps discarding the spatial information, and  is originally designed for texture synthesis~\mbox{%DIFAUXCMD
\cite{gatys2015texture}}%DIFAUXCMD
. The gram matrix at layer $l$ is given by:
}\begin{equation}
\DIFadd{G^lautoref(\mathcal{X}) = }{\DIFadd{F^lautoref(\mathcal{X})}} \DIFadd{\cdot }{\DIFadd{autoref( }{{\DIFadd{F^lautoref(\mathcal{X})}}} \DIFadd{)^T}}
\DIFadd{\label{eq:Gram_element}
}\end{equation}
\DIFadd{where ${G^lautoref(\mathcal{X})} \in {\mathcal{R}^{{M_l} \times {M_l}}}$, and $M_l$ denotes the number of filters in layer $l$. A multi-resolution style representation is obtained by combining the gram matrices at different layers: $\mathcal{S}autoref(\mathcal{X})=$\{$G^l autoref(\mathcal{X})$\} where $l\in$\{conv1\_1, conv2\_1, conv3\_1, conv4\_1, conv5\_1\}. This style information is useful in guiding the sketch synthesis, especially in the texture region such as hair and shadings around the nose, eyes and mouth. 
}{
\subsection{\DIFadd{Target Style Generation via Multi-resolution Grouping}}
}
%DIF >  \begin{figure}[htbp]
%DIF >  \centering
%DIF >  \includegraphics[width=0.85\linewidth]{pyramidcolumn.pdf}
%DIF >  \caption{Illustration of pyramid column. Feature maps of the final sketch, $A^{l}$, are divided into patches according to the size of the smallest feature map. A pyramid column $U_{ij}$ consists of feature map patches at different layer having the same grid indices $i$ and $j$. The input photo and the final sketch are divided into patches where centers $C_{ij}$ are denoted as dots. $U_{12}$ and $C_{12}$ are colored in red as an example.}
%DIF >  \label{fig:pyramidcolumn}
%DIF >  \end{figure}
\DIFadd{There usually does not exist a candidate image in the training set that perfectly matches a given test photo in style. We hence propose a patch based method to estimate the style of the final sketch. }\DIFaddend Denote the feature maps (of the $l$th layer) used to estimate the style of the final sketch by $A^{l}$. In our \DIFdelbegin \DIFdel{feature }\DIFdelend patch based method, we divide $A^{l}$ into a \DIFdelbegin \DIFdel{fixed size of grid . Due to the different feature map size}\DIFdelend \DIFaddbegin \DIFadd{grid where each grid cell (feature map patch) at layer $conv5\_1$ contains only one pixel. Since the pooling size is $2\times2$, we choose the sizes of feature map patches at the next lower layer twice of that at the current layer. Thus}\DIFaddend , the sizes of the feature \DIFaddbegin \DIFadd{map }\DIFaddend patches at layer $conv1\_1$, $conv2\_1$, $conv3\_1$, $conv4\_1$ and $conv5\_1$ are $16\times16$, $8\times8$, $4\times4$, $2\times2$ and $1\times1$\DIFdelbegin \DIFdel{. The photos and sketches are resized to $288\times288$ï¼Œ thus the size of grid is $18\times18$. Grouping feature }\DIFdelend \DIFaddbegin \DIFadd{, respectively. We introduce a multi-resolution feature descriptor by grouping feature }\DIFaddend map patches having the same grid \DIFdelbegin \DIFdel{indexes $(i, j)$ at different layers together , we get a pyramid feature column}\DIFdelend \DIFaddbegin \DIFadd{indices $i$, $j$ at different layer index $l$ together as a pyramid column, }\DIFaddend $U_{ij}$. \DIFaddbegin \DIFadd{The photos and sketches are resized so that both the number of rows and columns are dividable by $16$. $A^{l}$ can then be represented as a hyper-grid $U$ with the cells being $U_{ij}$. Similarly, we divide the test photo and the final sketch, respectively, into a grid with the same dimension as $U$. Denote the centers of these patches as $C_{ij}$. $U_{ij}$ is hence a multi-resolution feature descriptor for a sketch region centered at $C_{ij}$ (see Fig.~\ref{fig:pyramidcolumn}). 
}\DIFaddend To estimate $U_{ij}$, a sketch patch in the training set is fed to the VGG-Network and a pyramid column is composed \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{from }\DIFaddend the resulting feature maps. This process consists of two steps: (1) find a matching sketch patch \DIFdelbegin \DIFdel{$S_{ij}$ }\DIFdelend from the training set \DIFaddbegin \DIFadd{$S_{ij}$ }\DIFaddend for $U_{ij}$ and (2) feed $S_{ij}$ to VGG-Network and \DIFdelbegin \DIFdel{extract }\DIFdelend \DIFaddbegin \DIFadd{compose }\DIFaddend $U_{ij}$ from the resulting feature maps.\DIFaddbegin \par
%DIF >  \begin{figure*}[htbp]
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{boarder_example.pdf}
%DIF >  \caption{Find a matching patch for boarder cells. (a) The center $C_{ij}$ of a boarder cell is denoted as the red dot in the photo. (b) The region associated with $U_{ij}$ is the intersection of the blue square with the sketch. The distances between $C_{ij}$ to the intersection boarders are denoted by green lines. (c) $\tilde{C}_{ij}$ denoted as a yellow dot is located by examining photo similarity. (d) The distances of $\tilde{C}_{ij}$ to boarders of $S_{ij}$ equal to the distances of  $C_{ij}$ to boarders of intersection region in (b) such that the location of $\tilde{C}_{ij}$ in $S_{ij}$ is the same as the location of $C_{ij}$ in the region associated with $U_{ij}$.}
%DIF >  \label{fig:boarder_example}
%DIF >  \end{figure*}
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \begin{figure}[htbp]
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=0.85\linewidth]{img/pyramidcolumn.pdf}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Illustration of pyramid feature column. Feature maps of the final sketch $A^{l}$ are divided into a fixed $18\times18$ grid. A pyramid column $U_{ij}$ consists of feature map patches at different layer having the same grid indexes $(i, j)$.}}
%DIFAUXCMD
%DIF <  The input photo and the final sketch are divided into patches where centers $C_{ij}$ are denoted as dots. $U_{12}$ and $C_{12}$ are colored in red as an example.
%DIFDELCMD < \label{fig:pyramidcolumn}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < \begin{figure*}[htbp]
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=0.99\linewidth]{img/border_example.pdf}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Find a matching patch for boarder cells. (a) The center $C_{ij}$ of a boarder cell is denoted as the red dot in the photo. (b) The region associated with $U_{ij}$ is the intersection of the blue square with the sketch. The distances between $C_{ij}$ to the intersection boarders are denoted by green lines. (c) $\tilde{C}_{ij}$ denoted as a yellow dot is located by examining photo similarity. (d) The distances of $\tilde{C}_{ij}$ to boarders of $S_{ij}$ equal to the distances of  $C_{ij}$ to boarders of intersection region in (b) such that the location of $\tilde{C}_{ij}$ in $S_{ij}$ is the same as the location of $C_{ij}$ in the region associated with $U_{ij}$.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:boarder_example}
%DIFDELCMD < \end{figure*}
%DIFDELCMD < 

%DIFDELCMD < %%%
\paragraph{\DIFdel{Find Matching Sketch Patch}} 
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
%DIFDELCMD < \redn[Unlike previous works~\cite{wang2009face,zhou2012markov}, we examine the similarity of content patches to find a matching sketch $S_{ij}$ for $U_{ij}$. Compared with photo patches, the content patches are more easier to match. Given a test photo patch $\mathcal{I}_{ij}$, ]
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Inspired by previous works~\cite{wang2009face,zhou2012markov}, we examine the similarity in appearance of photo patches to find a matching sketch $S_{ij}$ for $U_{ij}$. Denote a patch centered at $C_{ij}$ in the test photo as $\Psi_{ij}$. If a patch centered at $\tilde{C}_{ij}$ in the $k$th photo in the training set has the smallest Euclidean distance from $\Psi_{ij}$ in term of color channels, a sketch patch, $S_{ij}$, centered at $\tilde{C}_{ij}$ is extracted from the sketch paired with the $k$th photo in the training set. The size and position of $S_{ij}$ are determined by the region associated with $U_{ij}$ which is calculated according to the architecture of the VGG-Network and location of $C_{ij}$. For non-boarder cells, $S_{ij}$ is a sketch patch of size $144\times144$ centered at $\tilde{C}_{ij}$. For cells near boarders, the region associated with $U_{ij}$ is the intersection of a sketch patch of size $144\times144$ centered at $C_{ij}$ with the whole sketch. Therefore, $S_{ij}$ is a patch from the $k$th sketch containing $\tilde{C}_{ij}$ such that the location of $\tilde{C}_{ij}$ in $S_{ij}$ is the same as the location of $C_{ij}$ in the region associated with $U_{ij}$ (see Fig.~\ref{fig:boarder_example}).\\
%DIFDELCMD < 

%DIFDELCMD < %%%
\paragraph{\DIFdel{Estimate Pyramid Column}} %DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin {\noindent \bf \DIFadd{Find Matching Sketch Patch}}\\
\DIFadd{Inspired by previous works~\mbox{%DIFAUXCMD
\cite{wang2009face,zhou2012markov}}%DIFAUXCMD
, we examine the similarity in appearance of photo patches to find a matching sketch $S_{ij}$ for $U_{ij}$. Denote a patch centered at $C_{ij}$ in the test photo as $\Psi_{ij}$. If a patch centered at $\tilde{C}_{ij}$ in the $k$th photo in the training set has the smallest Euclidean distance from $\Psi_{ij}$ in term of color channels, a sketch patch, $S_{ij}$, centered at $\tilde{C}_{ij}$ is extracted from the sketch paired with the $k$th photo in the training set. The size and position of $S_{ij}$ are determined by the region associated with $U_{ij}$ which is calculated according to the architecture of the VGG-Network and location of $C_{ij}$. For non-boarder cells, $S_{ij}$ is a sketch patch of size $144\times144$ centered at $\tilde{C}_{ij}$. For cells near boarders, the region associated with $U_{ij}$ is the intersection of a sketch patch of size $144\times144$ centered at $C_{ij}$ with the whole sketch. Therefore, $S_{ij}$ is a patch from the $k$th sketch containing $\tilde{C}_{ij}$ such that the location of $\tilde{C}_{ij}$ in $S_{ij}$ is the same as the location of $C_{ij}$ in the region associated with $U_{ij}$ (see Fig.~\ref{fig:boarder_example}).}\\
{\bf \DIFadd{Estimate Pyramid Column}}\\
\DIFaddend We compose an estimate for $U_{ij}$ from the resulting feature maps obtained by feeding $S_{ij}$ to the VGG-Network. Specifically, after feeding $S_{ij}$ to the VGG-Network, we form a hyper-grid $\tilde{U}$ on the resulting feature maps. The pyramid column corresponding to $\tilde{C}_{ij}$ is selected as an estimation to $U_{ij}$. This pyramid column is the cell that contains $\tilde{C}_{ij}$ when projecting $S_{ij}$ onto $\tilde{U}$.\par
After $A^{l}$ are estimated, we calculate the target gram matrices by: \{$G_{t}^l =A^{l} \cdot {autoref( {{A^{l}}} )^T}$\} where $l\in L_s$.

{
\DIFdelbegin \subsection{\DIFdel{Loss Function}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \section{\DIFadd{Combine Style with Content}}
\DIFaddend }
To generate a sketch that combines the content image with the estimated style, we adopt the methodology described in~\cite{gatys2015texture}. Specifically, we minimize a loss function consisting of a style loss, a content loss and a component loss. The style loss is defined as the difference between the gram matrix of the final sketch and the target gram matrix i.e.,
\begin{equation}
\mathcal{L}_{s} autoref( \mathcal{X} ) = \sum\limits_{l \in {L_s}} {\frac{1}{{M_l^2N_l^2}}autoref\| {{G^l}autoref( \mathcal{X} ) - G_t^l} \|_2^2} 
\label{eq:Gram_loss}
\end{equation}
where $N_l$ denotes the number pixels in the feature map at layer $l$. The content loss is defined based on the difference between the feature map of the sketch and that of the content image at layer conv1\_1:
\begin{equation}
\mathcal{L}_{c}autoref( \mathcal{X} ) = autoref\| {{F^{\rm{conv1\_1}}}autoref( \mathcal{X} ) - {F^{\rm{conv1\_1}}}autoref( \mathcal{C} )} \|_2^2.
\label{eq:Style_loss}
\end{equation}
Human is able to distinguish different people from key components such as eyes, nose and mouth, which indicates that these features are the most discriminative parts of a face. Specific styles or textures are usually used to emphasize these components, for example, sharp edges with shadings at the two sides of the nose are used to convey 3D information. To better transfer styles of these components, we employ a component loss to encourage the key component style of the final sketch being the same as the target key component style. Since two eyes are placed at fixed positions, the key components lie roughly within a rectangular region taking the positions of two eyes as vertices. Key component style is given by gram matrices calculated within feature map regions $\mathcal R$ corresponding to the key components. These regions are specified by hyper-grid cells $U_{ij}$ whose $C_{ij}$ are inside the rectangular region. More specifically, a target style for the key component is calculated: \{${\hat G}_{t}^l ={A}_{c}^l \cdot {autoref( {{{A}_{c}^l}} )^T}$\} where ${A}_{c}^l$ is the composed feature map patch in $A^{l}$ corresponding to the key components, $l\in L_s$. The style of the key components in final sketch ${\hat G}^lautoref( \mathcal{X} ) $ is calculated in exactly the same manner. The component loss is hence defined as:
\begin{equation}
\mathcal{L}_{k} autoref( \mathcal{X} ) = \sum\limits_{l \in {L_s}} {\frac{1}{{M_l^2{\hat N}_l^2}}autoref\| {{{\hat G}^l}autoref( \mathcal{X} ) - {\hat G}_t^l} \|_2^2} 
\label{eq:component_loss}
\end{equation}
where ${\hat N}_l$ denotes the number of pixels in the feature map region $\mathcal R$ at layer $l$. The total loss we minimize is 
\begin{equation}
\mathcal{L}_{t}autoref( \mathcal{X} ) = \alpha \mathcal{L}_{c} + \beta_1 \mathcal{L}_{s} + \beta_2 \mathcal{L}_{k}
\label{eq:Total_loss}
\end{equation}
where $\alpha$, $\beta_1$ and $\beta_2$ are the weighting factors for content, style and component losses respectively. The minimization is carried out using L-BFGS. Instead of using random noises, we use the content image as a starting point, which will make the optimization process converge much faster. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < ------------------------------------------------------------------------
\subsection{\DIFdel{Implementation Details}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Since the VGG-Network is originally designed for color images, while sketches are gray scale images }\DIFdelend \DIFaddbegin {
\section{\DIFadd{Experiments}}
}
%DIF >  \begin{figure*}[htbp]
%DIF >  \centering
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example1_gro.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example2_gro.pdf}
%DIF >  (a) Sketches by artist
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example1_FCNN.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example2_FCNN.pdf}
%DIF >  (b) FCNN
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example1_MRF.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example2_MRF.pdf}
%DIF >  (c) MRF
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example1_ours.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example2_ours.pdf}
%DIF >  (d) Ours
%DIF >  \end{minipage}
%DIF >  \caption{Examples of qualitative evaluation on CUHK benchmark datasets. (a) The sketches drawn by artists. (b) Results from feed-forward CNN based method~\cite{zhang2015end}. (c) Results from MRF based method~\cite{wang2009face}. (d) Our results.}
%DIF >  \label{fig:q_eval_1}
%DIF >  \end{figure*}
%DIF >  \begin{figure*}[htbp]
%DIF >  \centering
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example3_gro.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example4_gro.pdf}
%DIF >  (a) Sketches by artist
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example3_WMRF.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example4_WMRF.pdf}
%DIF >  (b) WMRF
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example3_Filter.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example4_Filter.pdf}
%DIF >  (c) SSD
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.245\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{example3_ours.pdf}
%DIF >  \includegraphics[width=0.99\linewidth]{example4_ours.pdf}
%DIF >  (d) Ours
%DIF >  \end{minipage}
%DIF >  \caption{Examples of qualitative evaluation on AR benchmark datasets. (a) The sketches drawn by artists. (b) Results from the WMRF method~\cite{zhou2012markov}. (c) Results from the SSD method~\cite{song2014real}. (d) Our results.}
%DIF >  \label{fig:q_eval_2}
%DIF >  \end{figure*}
\DIFadd{In all experiments, we resize test photos and photo-sketch pairs in the training set to a fixed size of $288\times288$. The final sketch is obtained by resizing the resulting sketch back to the original size. The size of $\Psi$ is $48\times48$}\DIFaddend , \DIFdelbegin \DIFdel{we modify the first layer of VGG-Network for gray scale images by setting the filter weights to
}\begin{displaymath}
\DIFdel{W^{k} = W^{k}_r+W^{k}_g+W^{k}_b
\label{eq:VGG_weights}
}\end{displaymath}
%DIFAUXCMD
\DIFdel{where $W^{k}_r$, $W^{k}_g$, }\DIFdelend \DIFaddbegin \DIFadd{$\alpha=0.004$, $\beta_1=1$ }\DIFaddend and \DIFdelbegin \DIFdel{$W^{k}_b$ are weights of the $k$th filter in the first convolutional layer for the R, G and B channels respectively, and $W^{k}$ is the weight of the $k$th filter in the first convolutional layer of our modified network. 
}\DIFdelend \DIFaddbegin \DIFadd{$\beta_2=0.1$. Since generating the final sketch involves an iterative optimization process, the gradient of the final sketch with respect to the content image is not easy to calculate, which prohibits training the content network in an end-to-end manner.  The training is therefore carried out by minimizing the squared loss between generated content images and sketches drawn by artists. We evaluate the performance of the proposed method against other state-of-the-art methods on the CUHK student dataset~\mbox{%DIFAUXCMD
\cite{wang2009face} }%DIFAUXCMD
and the AR dataset~\mbox{%DIFAUXCMD
\cite{martinez1998r}}%DIFAUXCMD
. For AR dataset, we use the leave-one-out strategy as conducted in previous studies~\mbox{%DIFAUXCMD
\cite{song2014real,wang2009face}}%DIFAUXCMD
. We compare the results of our method against those of the MRF method~\mbox{%DIFAUXCMD
\cite{wang2009face}}%DIFAUXCMD
, weighted MRF (WMRF) method~\mbox{%DIFAUXCMD
\cite{zhou2012markov}}%DIFAUXCMD
, feed-forward CNN (FCNN) method~\mbox{%DIFAUXCMD
\cite{zhang2015end} }%DIFAUXCMD
and Filtering based method (SSD)~\mbox{%DIFAUXCMD
\cite{song2014real}}%DIFAUXCMD
.
}{
\subsection{\DIFadd{Sketch Recognition}}
}
\DIFadd{Sketch synthesis methods are usually evaluated quantitatively  via the face sketch recognition task~\mbox{%DIFAUXCMD
\cite{song2014real,wang2009face,zhang2015end,zhou2012markov}}%DIFAUXCMD
. If an algorithm achieves higher sketch recognition rates, it suggests that this method is more effective in synthesizing sketches. We adopt the widely used PCA based recognition method with ``rank-1 (R1)'', ``rank-5 (R5)'' and ``rank-10 (R10)'' criteria ~\mbox{%DIFAUXCMD
\cite{wang2009face} }%DIFAUXCMD
where ``rank $n$'' measures the rate of the correct answer in the top $n$ best matches. The results of different methods are shown in Table~\ref{tab:reg_percentage}. Our method achieves the best performance against all other methods in the ``R1'' and ``R5'' tests. There may be two reasons behind this. First, since the test photos are different from those in the training set, we do not expect to find suitable patches from the training set for the target patches. The remedy of linearly combining the patches as in~\mbox{%DIFAUXCMD
\cite{zhou2012markov} }%DIFAUXCMD
will introduce over smoothing and blurring artifacts, which are also observed in the filtering like approach~\mbox{%DIFAUXCMD
\cite{song2014real}}%DIFAUXCMD
. Being a generative model, the content network learns the modality transformation locally between photos and sketches, such as transferring edges in photos to strokes in sketches. This helps generating sketches for structures not existing in the training set. Second, our model explicitly minimizes the difference by the style between the synthesized sketches and sketches drawn by artists, while this difference is ignored by all other methods. The performance of all methods in the ``R10'' test are fairly the same. 
}\begin{table}
\small
\begin{tabular}{C{0.2cm}C{0.2cm}C{0.35cm}C{0.1cm}C{0.35cm}C{0.1cm}C{0.35cm}C{0.1cm}C{0.35cm}C{0.1cm}C{0.35cm}C{0.1cm}C{0.35cm}}
\hline
\multicolumn{2}{c|}{\multirow {2}{*}{Methods}} & \multicolumn{5}{c}{AR} & & \multicolumn{5}{c}{CUHK} \\
\cline{3-7} \cline{9-13} \multicolumn{2}{c|}{} & \DIFaddFL{R1 }& & \DIFaddFL{R5 }& & \DIFaddFL{R10 }& & \DIFaddFL{R1 }& & \DIFaddFL{R5 }& & \DIFaddFL{R10   }\\
\hline
\multicolumn{2}{c|}{FCNN} & \DIFaddFL{- }& & \DIFaddFL{- }& & \DIFaddFL{- }& & \DIFaddFL{81\% }& & \DIFaddFL{96\% }& & \DIFaddFL{97\%   }\\
\multicolumn{2}{c|}{MRF}  & \DIFaddFL{97.5\% }& & \DIFaddFL{97.5\% }& & \DIFaddFL{\textcolor{red}{100\%} }& & \DIFaddFL{83\% }& & \DIFaddFL{96\% }& & \DIFaddFL{96\% }\\
\multicolumn{2}{c|}{WMRF} & \DIFaddFL{97.5\% }& & \DIFaddFL{97.5\% }& & \DIFaddFL{\textcolor{red}{100\%} }& & \DIFaddFL{83\% }& & \DIFaddFL{97\% }& & \DIFaddFL{98\%  }\\
\multicolumn{2}{c|}{SSD}  & \DIFaddFL{96.7\% }& & \DIFaddFL{97.5\% }& & \DIFaddFL{\textcolor{red}{100\%} }& & \DIFaddFL{\textcolor{red}{87\%} }& & \DIFaddFL{97\% }& & \DIFaddFL{98\%   }\\
\multicolumn{2}{c|}{Ours} & \DIFaddFL{\textcolor{red} }{\DIFaddFL{98.4\%}} & & \DIFaddFL{\textcolor{red} }{\DIFaddFL{98.4\%}} & & \DIFaddFL{\textcolor{red} }{\DIFaddFL{100\%}} & & \DIFaddFL{\textcolor{red} }{\DIFaddFL{87\%}} & & \DIFaddFL{\textcolor{red} }{\DIFaddFL{98\%}} & & \DIFaddFL{\textcolor{red} }{\DIFaddFL{99\%}}   \\
\hline
\end{tabular}
\caption{\DIFaddFL{Recognition rate on benchmark datasets. The best performance is colored in red.}}
\label{tab:reg_percentage}
\end{table}
\DIFaddend 

%DIF < ------------------------------------------------------------------------
\DIFdelbegin \section{\DIFdel{Final copy}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{table*}[htbp]
\begin{center}
{\small
\begin{tabular}{ccccccccccccc}
\hline
\multicolumn{2}{c|}{\multirow {2}{*}{Methods}} &\multicolumn{5}{c}{AR} & & \multicolumn{5}{c}{CUHK} \\
\cline{3-7} \cline{9-13} \multicolumn{2}{c|}{} & \DIFaddFL{conv1\_1 }& \DIFaddFL{conv2\_1 }& \DIFaddFL{conv3\_1 }& \DIFaddFL{conv4\_1 }& \DIFaddFL{conv5\_1 }& &
 \DIFaddFL{conv1\_1 }& \DIFaddFL{conv2\_1 }& \DIFaddFL{conv3\_1 }& \DIFaddFL{conv4\_1 }& \DIFaddFL{conv5\_1  }\\
\hline
\multicolumn{2}{c|}{FCNN} & \DIFaddFL{- }& \DIFaddFL{- }& \DIFaddFL{- }& \DIFaddFL{- }& \DIFaddFL{- }& & { \DIFaddFL{$0.009$}} &{ \DIFaddFL{$0.110$}} & {\DIFaddFL{$0.080$}} & {\DIFaddFL{$9.43$}} & {\DIFaddFL{$1.49$}}  \\
\multicolumn{2}{c|}{MRF} & \DIFaddFL{0.0043 }& \DIFaddFL{0.009 }& {\DIFaddFL{$0.033$}}  & {\DIFaddFL{$0.12$}} & \DIFaddFL{0.28 }& & { \DIFaddFL{$0.010$}} &{ \DIFaddFL{$0.014$}} & {\DIFaddFL{$0.047$}} & \DIFaddFL{$0.13$ }& \DIFaddFL{$0.18$  }\\
\multicolumn{2}{c|}{WMRF} & \DIFaddFL{0.0053 }& \DIFaddFL{0.027 }& {\DIFaddFL{$0.085$}}  & {\DIFaddFL{$0.19$}} & \DIFaddFL{0.29 }& & { \DIFaddFL{$0.010$}} &{ \DIFaddFL{$0.052$}} & {\DIFaddFL{$0.052$}} & {\DIFaddFL{$0.27$}} & {\DIFaddFL{$0.19$}}  \\
\multicolumn{2}{c|}{SSD} & \DIFaddFL{0.0056 }& \DIFaddFL{0.036 }& {\DIFaddFL{$0.110$}} & {\DIFaddFL{$1.90$}} & \DIFaddFL{0.28 }& & { \DIFaddFL{$0.009$}} &{ \DIFaddFL{$0.102$}} & {\DIFaddFL{$0.070$}} & {\DIFaddFL{$3.32$}} & {\DIFaddFL{$0.24$}}  \\
\multicolumn{2}{c|}{Ours} & {\color{red} \DIFaddFL{0.0035}} & {\color{red} \DIFaddFL{0.008}} & {\color{red} \DIFaddFL{$0.029$}} & {\color{red} \DIFaddFL{$0.08$}} & {\color{red} \DIFaddFL{$0.17$}} & & {\color{red}\DIFaddFL{$0.007$}} &{\color{red} \DIFaddFL{$0.012$}} & {\color{red} \DIFaddFL{$0.033$}} & {\color{red} \DIFaddFL{$0.07$}} & {\color{red} \DIFaddFL{$0.12$}}  \\
\hline
\end{tabular}
}
\end{center}
\caption{\DIFaddFL{Averaged NGMD value of different methods at different level on AR and CUHK datasets.}}
\label{tab:NGMD}
\end{table*}
{
\subsection{\DIFadd{Style Transfer}}
}
%DIF >  \begin{figure}[htbp]
%DIF >  \centering
%DIF >  \begin{minipage}[t]{0.49\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{wo_region.pdf}
%DIF >  (a) $\beta_2=0$
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.49\linewidth}
%DIF >  \includegraphics[width=0.99\linewidth]{w_region.pdf}
%DIF >  \centering
%DIF >  (b) $\beta_2=0.1$
%DIF >  \end{minipage}
%DIF >  \caption{Comparison between results with and without the $\mathcal{L}_{k} $ regulation. (a) When turning off the component loss, the nose suffers from the distortion. (b) Thanks to the component loss, a more favorable result is obtained.}
%DIF >  \label{fig:region_effect}
%DIF >  \end{figure}
\DIFaddend 

\DIFdelbegin \DIFdel{You must include your signed IEEE copyright release form when you submit
your finished paper. We MUST have this form before your paper can be
published in the proceedings. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Please direct any questions to the production editor in charge ofthese
proceedings at the IEEE Computer Society Press: Phone (714)821-8380, or
Fax (714)761-1784}\DIFdelend %DIF >  \begin{figure*}[htbp]
%DIF >  \centering
%DIF >  \begin{minipage}[t]{0.16\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{inf_alpha.pdf}
%DIF >  $\beta_1  = 0 $\\
%DIF >  $\beta_2  = 0 $
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.16\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{4_alpha.pdf}
%DIF >  $\beta_1  = 10^{-3} $\\
%DIF >  $\beta_2  = 10^{-4} $
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.16\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{3_alpha.pdf}
%DIF >  $\beta_1  = 10^{-2} $\\
%DIF >  $\beta_2  = 10^{-3} $
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.16\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{2_alpha.pdf}
%DIF >  $\beta_1  = 0.1 $\\
%DIF >  $\beta_2  = 0.01 $
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.16\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{1_alpha.pdf}
%DIF >  $\beta_1  = 1 $\\
%DIF >  $\beta_2  = 0.1 $
%DIF >  \end{minipage}
%DIF >  \begin{minipage}[t]{0.16\linewidth}
%DIF >  \centering
%DIF >  \includegraphics[width=0.99\linewidth]{0_alpha.pdf}
%DIF >  $\beta_1  = 10^{3} $\\
%DIF >  $\beta_2  = 10^{2} $
%DIF >  \end{minipage}
%DIF >  \caption{Final results for sketch synthesis with fixed $\alpha=0.004$ and different $\beta_1$, $\beta_2$ values.}
%DIF >  \label{fig:alphg_effect}
%DIF >  \end{figure*}
% \DIFaddbegin \DIFadd{Human can often be able to tell whether a sketch is generated by algorithms or drawn by artists just from a quick glance without examining details. This indicates that there exists a big gap between the style of algorithm-generated sketches and that of those drawn by artists. To quantitatively evaluate the style similarity between a generated sketch and the sketch drawn by a real artist, we employ the normalized gram matrix difference (NGMD) between the generated sketch $\mathcal{X}$ and the drawn sketch ${\tilde {\mathcal{X}}}$:
% }\MATHBLOCKequation{
% {\DIFadd{D^l_s}}\DIFadd{autoref( }{\DIFadd{\mathcal{X},\tilde }{\DIFadd{\mathcal{X}}}} \DIFadd{) = \frac{{autoref\| {{G^l}autoref( \mathcal{X} ) - {G^l}autoref( {\tilde {\mathcal{X}}} )} \|_2^2}}{{autoref\| {{G^l}autoref( {\tilde {\mathcal{X}}} )} \|_2^2}}
% \label{eq:style_exp}
% }}
\DIFadd{where $l\in L_s$. The smaller the NGMD value is, the more similar $\mathcal{X}$ and ${\tilde {\mathcal{X}}}$ are. Thanks to our style transfer, our results have the smallest NGMD value among all methods under test (see Table 2). It is therefore obvious that our results are the closest to the sketches drawn by artists. 
Qualitative evaluation results are shown in Fig.~\ref{fig:q_eval_1} and Fig.~\ref{fig:q_eval_2}, where we find that our method provides much clear boundaries and better texture in regions containing hairs and shadings. The results of~\mbox{%DIFAUXCMD
\cite{zhang2015end} }%DIFAUXCMD
can roughly outline the sketches, but they do not look like sketches in style. This is also indicated by a high NGMD in Table~\ref{tab:NGMD}. The results of MRF~\mbox{%DIFAUXCMD
\cite{wang2009face} }%DIFAUXCMD
and WMRF~\mbox{%DIFAUXCMD
\cite{zhou2012markov} }%DIFAUXCMD
are more similar to hand drawn sketches in style since these exemplar based approaches use patches of hand drawn sketches to make up the target sketch. That is why they have a smaller NGMD value. Inheriting the ability of denoising, the filtering based approach~\mbox{%DIFAUXCMD
\cite{song2014real} }%DIFAUXCMD
is good at suppressing noises in the results. However, it is also likely for this method to over-smooth the results, which will deteriorate the texture and even introduce blurring artifacts. 
}{
\subsection{\DIFadd{Effectiveness of the model}}
}
\DIFadd{The loss function we minimize during the generation of sketches contains three terms for content, style and key components respectively. The term $\mathcal{L}_{k} $ regularizes the results by encouraging the style extracted from the key component regions in the training set to be placed into the key components region of the results, which helps generate better results around these components (see Fig.~\ref{fig:region_effect}). To better understand how style influences the final sketch, we smoothly change the emphasis on style by adjusting $\beta_1$ and $\beta_2$ while keeping $\alpha$  fixed. Fig.~\ref{fig:alphg_effect} indicates that the sketch with style transfered contains more texture and is more like a drawn sketch. The Theano implementation of the proposed method takes approximately $100$ seconds to generate a sketch on a GeForce GTX TITAN X platform. The bottle neck lies in the style transfer which requires feeding $\mathcal{X}$ to the VGG-Network to estimate targeting feature maps and to calculate the gradient of Eq.~(\ref{eq:Total_loss}), which is computationally intensive. 
}{
\section{\DIFadd{Conclusion and Future Work}}
}
\DIFadd{This paper proposed a novel face sketch synthesis method inspired by the procedure of artists drawing sketches. In our method, the outline of the face is delineated by a content network and the style extracted from sketches drawn by artists are transferred to generate a final sketch. Quantitative evaluations on face sketch recognition and style similarity measure demonstrate the effectiveness of the proposed algorithm for face sketch synthesis and style transferring. Our future work will investigate accelerating technique to reduce the running time and achieve real time face sketch synthesis with style transfer}\DIFaddend .


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}